---
title: "dsUpload"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{dsUpload}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# DataSHIELD upload tools

## Background
The aim of the EU Child Cohort Network is to bring together data from existing child cohorts into one open and sustainable, multi-disciplinary network.
To ensure sure that the EU Child Cohort Network is both open and sustainable, the consortia are using the data-sharing platform DataSHIELD. 

Participating cohorts must harmonise data, following the data harmonisation manuals. Secondly, perform quality-control checks on their harmonised data. Thirdly, upload descriptions of harmonisation to the cohort catalogue. The last step is uploading the data into the DataSHIELD backends. The guide below guides you through the process.

## Installation
Reference documentation:

- [dsUpload](https://lifecycle-project.github.io/ds-upload)
- [Armadillo](http://molgenis.github.io/molgenis-r-armadillo)
- [Opal](http://opaldoc.obiba.org/en/latest/r-user-guide/datashield.html)

You can install the package by executing the following command:

```{r, install the package, eval = FALSE}
install.packages("dsUpload", repos=c('https://registry.molgenis.org/repository/R/', 'https://cran.datashield.org'), dependencies = TRUE)
```

When you want to use it you need to load it.

```{r, setup}
# load the package
library(dsUpload)
```

## Troubleshooting
Check: [troubleshooting guide](https://github.com/lifecycle-project/ds-upload/blob/master/TROUBLESHOOTING.md)

## Usage
To simplify the upload and importing of data dictionaries this package is written to import and upload the data dictionaries and data in one run. When running the package, you need to specify the data dictionary version and the data input file. When you use data formats other than CSV use need to specify the data format as well

**Prerequisites**

- Upload core variables
  
  Merge all the variables that are obtained in the dictionary of the core variables. 
  So in general that means merge the data of WP1 and WP3 into one set.

- Upload outcome variables

  Merge all the variables that are obtained in the dictionary of the outcome variables. 
  So in general this means merge the data of WP4, WP5 and WP6 into one set. 

### Using Armadillo
Please following the instruction below to upload the core and outcome variables in the Aramdillo.

```{r, create Armadillo dataframe}
login_data <- data.frame(
  server = "https://armadillo.test.molgenis.org", 
  driver = "ArmadilloDriver")
```

```{r, login to the Armadillo}
# login to the Armadillo server
du.login(login_data = login_data)
```

```{r, upload and import the core variables for the Armadillo}
# upload the data into the DataSHIELD backend
# these are the core variables
# be advised the default input format is 'CSV'
# you can use STATA, SPSS, SAS and CSV's as source files
du.upload(
  cohort_id = 'gecko', 
  dict_version = '2_1', 
  dict_kind = 'core', 
  data_version = '1_0', 
  data_input_format = 'CSV',
  data_input_path = 'https://github.com/lifecycle-project/ds-upload/blob/master/inst/examples/data/WP1/data/all_measurements_v1_2.csv?raw=true',
  run_mode = "non_interactive"
)
```

```{r, upload and import the outcome variables for Armadillo}
# upload the outcome variables
du.upload(
  cohort_id = 'gecko', 
  dict_version = '1_1', 
  dict_kind = 'outcome', 
  data_version = '1_0', 
  data_input_format = 'CSV',
  data_input_path = 'https://github.com/lifecycle-project/ds-upload/blob/master/inst/examples/data/WP6/nd_data_wp6.csv?raw=true',
  run_mode = "non_interactive"
)
```

### Using Opal
A video guiding you through the process can be found here:

Check youtube channel: [upload data dictionaries and data into Opal](https://youtu.be/fUrpbstPCsU)

Alternatively, execute these commands in your R-console:

```{r, create Opal dataframe}
login_data <- data.frame(
  server = "https://opal.edge.molgenis.org", 
  user = "administrator", 
  password = "ouf0uPh6",
  driver = "OpalDriver")
```

```{r, login to the Opal}
# login to the DataSHIELD backend
du.login(login_data = login_data)
```

```{r, upload and import the core variables for the Opal}
# upload the data into the DataSHIELD backend
# these are the core variables
# be advised the default input format is 'CSV'
# you can use STATA, SPSS, SAS and CSV's as source files
du.upload(
  cohort_id = 'gecko', 
  dict_version = '2_1', 
  dict_kind = 'core', 
  data_version = '1_0', 
  data_input_format = 'CSV',
  data_input_path = 'https://github.com/lifecycle-project/ds-upload/blob/master/inst/examples/data/WP1/data/all_measurements_v1_2.csv?raw=true',
  run_mode = "non_interactive"
)
```
```{r, upload and import the outcome variables for Opal}
# upload the outcome variables
du.upload(
  cohort_id = 'gecko', 
  dict_version = '1_1', 
  dict_kind = 'outcome', 
  data_version = '1_0', 
  data_input_format = 'CSV',
  data_input_path = 'https://github.com/lifecycle-project/ds-upload/blob/master/inst/examples/data/WP6/nd_data_wp6.csv?raw=true',
  run_mode = "non_interactive"
)
```

>**IMPORTANT**: You can run this package for the core variables and for the outcome variables. Each of them requires changing some parameters in the function call. So dict_kind specific 'core' or 'outcome' variables and dict_version specifies the data dictionary version (check the changelogs here: https://github.com/lifecycle-project/ds-dictionaries/tree/master/changelogs). 

>**IMPORTANT** You can specify your upload format! So you do not have to export to CSV first. Supported upload formats are: 'SPSS', 'SAS', 'STATA' and 'CSV'.

#### Import the data
If you run these commands, your data will be uploaded to the DataSHIELD backend. If you use Opal, you can now import these data into the tables manually.

A video guiding you through the process can be found here: [import data into Opal](https://youtu.be/BKDnIAQzpdY)

Alternatively, execute these actions for Opal:

1. Navigate to the Opal webinterface
2. Login with you credentials
3. Select "Projects" 
4. Select "lc_#cohort#_#dict_kind#_x_x" 
5. Select "Import"
6. Choose the CSV-file
7. Select the target table (depending on your choice regarding the file you have chosen)
8. Click on "Next"
9. Click on "Next"
10. Determine that you all variables are matched, otherwise you need to fix your source-data first

> **IMPORTANT**: make sure no NEW variables are introduce
11. Click on "Finish"
12. Check the "Task logs" (on the left side of the screen, in the icon bar)

It will match your data dictionary and determine which variables are matched or not. You can re-upload the source files as often as needed.
